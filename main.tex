\documentclass{beamer}
\usetheme[secheader]{Boadilla}
\usepackage{todonotes}
\defbeamertemplate*{footline}{Boadilla}
{
    \leavevmode%
    \hbox{%
%   \begin{beamercolorbox}[wd=.333333\paperwidth,ht=2.25ex,dp=1ex,right]{author in head/foot}%
%     \usebeamerfont{author in head/foot}\insertshortauthor~~(\insertshortinstitute)
%   \end{beamercolorbox}%
%   \begin{beamercolorbox}[wd=.333333\paperwidth,ht=2.25ex,dp=1ex,right]{title in head/foot}%
%     \usebeamerfont{title in head/foot}\insertshorttitle
%   \end{beamercolorbox}%
        \begin{beamercolorbox}[wd=1\paperwidth,ht=2.25ex,dp=1ex,right]{date in head/foot}%
%     \usebeamerfont{date in head/foot}\insertshortdate{}\hspace*{2em}
            \insertframenumber{}\hspace*{2ex}
        \end{beamercolorbox}}%
    \vskip0pt%
}

\usepackage{subfigure}



\usepackage{comment}
% wow this is a hack that lets you have more definitions in latex.
% see http://www.tex.ac.uk/cgi-bin/texfaq2html?label=noroom
\usepackage{natbib}
\usepackage{etex}
\usepackage{pgffor}
\usepackage[utf8]{inputenc}
%\usepackage[cyr]{aeguill}
%\reserveinserts{28}
\usepackage{tikz}
\usepackage[customcolors]{hf-tikz}
%\usepackage[utf8]{inputenc}
%\mode<presentation>{\usetheme{Caltech}}

\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{multimedia}
\usepackage{color}
\usepackage{esint}
\usepackage{stmaryrd}
\usepackage{tabularx}
\usepackage{multirow}
\usepackage[squaren]{SIunits}
\usepackage{graphicx}
\usepackage{diagbox}
\usepackage{pdfpages}
\usepackage{dsfont}
\usepackage{xcolor}
\usepackage{soul}
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}


\graphicspath{{./img/}}

\newcommand{\cplus}{\colorbox{green}{($+$)} }
\newcommand{\cmoins}{\colorbox{red}{($-$)} }
\newcommand{\cmean}{\colorbox{yellow}{($\pm$)}}

\newcommand{\mathcolorbox}[2]{\colorbox{#1}{$\displaystyle #2$}}

\input{mathcommands.tex}
%\input{../glossary.tex}
\input{symbol.tex}

\beamertemplatenavigationsymbolsempty

\author[shortname]{Nicolas Carrara}
\institute[shortinst]{University of Toronto}

\title[ROMA]{ROMA: Multi-Agent RL with Emergent Roles}
\subtitle{Tonghan Wang, Heng Dong, Victor Lesser and Chongjie Zhang}

\date{April 15, 2020}
\begin{document}
    \begin{frame}
        \maketitle
        \centering
    \end{frame}

    \begin{frame}{Cooperative \textbf{M}ulti \textbf{A}gent \textbf{R}einforcement \textbf{L}earning}

        \begin{figure}
            \includegraphics[width=0.75\linewidth]{img/marl.pdf}
            %\caption{Comparison of our method against baseline algorithms. Results for more maps can be found in Appendix C.1.}\label{fig:performance-baselines}
        \end{figure}
        \pause
        \begin{exampleblock}{}
            Can be be cast as a Dec-POMDP
        \end{exampleblock}

    \end{frame}

    \begin{frame}{\textbf{Dec}entralized \textbf{P}artially \textbf{O}bservable \textbf{M}arkov \textbf{D}ecision \textbf{P}rocess}


        \begin{columns}
            \pause
            \begin{column}{0.45\textwidth}
                \begin{block}{Definition}
                    $G=\left\langle S,U,P,r,Z,O,n,\gamma\right\rangle$
                    \begin{itemize}
                        \item[$n$] the number of agents.
                        \item[$S$] the true-state space.
                        \item[$U$] the joint action space.
                        \item[$P$] the transition function.
                        \item[$r$] the reward function.
                        \item[$Z$] the observation space.
                        \item[$O$] the observation function.
                        \item[$\gamma$] the discount factor.
                    \end{itemize}
                \end{block}
            \end{column}
            \pause
            \begin{column}{0.50\textwidth}
                \begin{block}{Solving a Dec-POMDP}
                    \begin{itemize}
                        \item[$\tau^a$] action-observation history.
                        \item[$\pi^a$] the stochastic local policy
                        \item[$\pi$] the stochastic joint policy.
                        \item[$R_t$] $=\sum^{\infty}_{i=0}\gamma^ir_{t+i}$ the return.
                    \end{itemize}
                \end{block}
            \end{column}
        \end{columns}

    \end{frame}

    \begin{frame}{Two extremes solutions}
        \pause
        \begin{block}{Cast as an MDP.}
            \begin{itemize}
                \item[] \cplus Optimal solution.
                \item[] \cmoins Action space untractable.
                \item[] \cmoins Assume the environment is fully observable.
                %\item Q-learning, DQN, IMPALA ...
            \end{itemize}
        \end{block}
        \pause
        \begin{block}{Independent local MDPs.}
            \begin{itemize}
                \item[] \cplus Tractable.
                \item[] \cplus Local views.
                \item[] \cmoins Not optimal.
                \item[] \cmoins May not converge (non stationarity).
                %\item IQL ...
            \end{itemize}
        \end{block}
    \end{frame}

    \begin{frame}{Mixing architectures}

        Based on \textbf{C}entralized \textbf{T}raining \textbf{D}ecentralized \textbf{E}xecution.

        \begin{columns}
            \pause
            \begin{column}{0.5\textwidth}
                \begin{figure}
                    \includegraphics[width=\linewidth]{img/mixing_network.pdf}
                    %\caption{Comparison of our method against baseline algorithms. Results for more maps can be found in Appendix C.1.}\label{fig:performance-baselines}
                \end{figure}
            \end{column}
            \pause
            \begin{column}{0.5\textwidth}  %%<--- here
                \begin{block}{Example}
                    \textbf{V}alue \textbf{D}ecomposition \textbf{N}etworks.

                    \begin{equation}
                        Q_{tot}(\boldsymbol{\tau}, \mathbf{u}) = \sum_{i=1}^n Q_i (\tau^i, u^i;\theta^i)\nonumber.
                    \end{equation}
                \end{block}
            \end{column}
        \end{columns}

    \end{frame}

    \begin{frame}{Q-mix}
        \vspace{-0.5cm}
        \begin{columns}
            \pause
            \begin{column}{0.62\textwidth}
                \begin{block}{Generalisation of $Q_{tot}$ factorisation.}
                    \begin{equation}
                        \argmax_{\mathbf{u}}Q_{tot}(\boldsymbol{\tau}, \mathbf{u}) =
                        \begin{pmatrix}
                            \argmax_{u^1}Q_1(\tau^1, u^1)   \\
                            \vdots \\
                            \argmax_{u^n}Q_n(\tau^n, u^n) \\
                        \end{pmatrix}\nonumber
                    \end{equation}
                \end{block}
            \end{column}
            \pause
            \begin{column}{0.33\textwidth}  %%<--- here
                \begin{minipage}[c][0.5\textheight][c]{\linewidth}
                    \begin{block}{Enforcing monotonicity}
                        \begin{equation}
                            \frac{\partial Q_{tot}}{\partial Q_a}  \geq 0,~ \forall a \in A. \nonumber
                        \end{equation}
                    \end{block}
                \end{minipage}
            \end{column}
        \end{columns}

        \pause
        \begin{center}
            \includegraphics[width=0.75\linewidth]{img/QMIX.png}
        \end{center}


    \end{frame}

    \begin{frame}{ROMA}

        \pause
        \begin{alertblock}{Limit}
            Mixing architectures are not sufficient for complex tasks with subtasks.
        \end{alertblock}
        \pause
        \begin{exampleblock}{Solution:  \textbf{R}ole \textbf{O}riented \textbf{M}ulti-\textbf{A}gents RL.}
            Define and assign roles/skills to agents.
            \begin{itemize}
                \item[$\rightarrow$] Construction of a stochastic role embedding space.
            \end{itemize}
        \end{exampleblock}

    \end{frame}

    \begin{frame}{The space}
        \begin{figure}
            \includegraphics[height=0.35\linewidth]{img/10m_vs_11m-g4.pdf}\hfill
            \includegraphics[height=0.35\linewidth]{img/10m_vs_11m-r4.pdf}\hfill
            %\caption{Visualization of our learned role representations at a timestep.
            %The blue agent has the maximum health, while the red ones are dead.
            %The corresponding policy is that agent 6 moves towards enemies to take on more firepower,
            %so that more seriously injured agents are protected.
            %Roles can change adaptively and will aggregate according to responsibilities that are compatible
            %with individual characteristics, such as location, agent type, health, etc.}\label{fig:teaser}
        \end{figure}
        \pause
        \begin{block}{}
            How to construct this space? Through regularizations of the loss function.
        \end{block}
    \end{frame}

    \begin{frame}{Some notations}

        % TODO maybe put a vienne diagram

        \begin{itemize}
            \item $i$: agent $i$.
            \item $\mathcal{L}$: a loss.
            \item $H(X)$: entropy.
            \item $\mathcal{CE}(X\|Y)$: cross-entropy
            \item $I(X;Y)$: mutual Information.
            \item $I(X;Y|Z)$ conditional mutual information.

        \end{itemize}

    \end{frame}

    \begin{frame}{The loss function}
        To define the loss, those roles properties must be fullfield:

        \begin{itemize}
            \item Dynamic;
            \item Versatile;
            \item Identifiable;
            \item Specialized.
        \end{itemize}
    \end{frame}

    \begin{frame}{Dynamic}
        Two complementary aspects:

        \begin{itemize}
            \item Roles encoding in a stochastic embedding space.
            \begin{itemize}
                \item $\rho_i \sim \normal(\mu_{\rho_i},\sigma_{\rho_i})$
            \end{itemize}
            \item Role are dictacted by a local observation.
            \begin{itemize}
                \item $(\mu_{\rho_i},\sigma_{\rho_i}) = f(o_i|\theta_{\rho})$
            \end{itemize}
        \end{itemize}

    \end{frame}

    \begin{frame}{Identifiable and versatile ($\mathcal{L}_{I}$)}

        \begin{itemize}
            \item Versatility: given an observation, roles should be diverse.
            \begin{itemize}
                \item Maximize $\mathcolorbox{yellow}{H(\rho_i|o_i)}$
            \end{itemize}
            \pause\item Identifiability: a trajectory (behavior) must be identifiable by a role.
            \begin{itemize}
                \item Minimize $\mathcolorbox{yellow}{H(\rho_i|\tau_i, o_i)}$
            \end{itemize}
        \end{itemize}
        \pause
        \begin{exampleblock}{Solution}
            maximize $\mathcolorbox{yellow}{I(\tau_i;\rho_i|o_i)}$.
        \end{exampleblock}
%        \pause
%        \begin{block}{In other words}
%            For a given agent and an observation, we enforce a bijection between roles and trajectories.
%        \end{block}
        \pause
        \begin{alertblock}{Limits}
            Untractable.
        \end{alertblock}
    \end{frame}
    \begin{frame}{Identifiable and versatile ($\mathcal{L}_{I}$)}

        \begin{exampleblock}{Solution (\colorbox{green}{variational posterior estimator $q_\xi$})}

            $\mathcal{L}_I(\theta_\rho, \xi) = \mathbb{E}_{(\tau^{t-1}_i, o^t_i)\sim D}\big[\mathcal{CE}[p(\rho^{t}_i | o^t_i)\| \mathcolorbox{green}{q_\xi(\rho^{t}_i | \tau^{t-1}_i, o^t_i)}] - H(\rho^{t}_i | o^t_i)\big]$

        \end{exampleblock}

%        \todo{(talk about $q_\eta$ ?)}
        \pause
        \begin{block}{}
            But what about roles \textbf{across} agents ?
        \end{block}

    \end{frame}

    \begin{frame}{Specialization ($\mathcal{L}_{D}$)}
        To encourage subtasks specialization:
        \pause
        % and thus can share their experiences to improve performances
        \begin{exampleblock}{Solution}
            Maximize $\mathcolorbox {yellow}{I(\rho_i;\tau_j| o_j)}$
        \end{exampleblock}
        \pause
        \begin{alertblock}{Limit}
            All agents will have the same role (and policy)
        \end{alertblock}
    \end{frame}
%    \begin{frame}{Specialization ($\mathcal{L}_{D}$)}
%        \begin{exampleblock}{Solution (disimilarity model)}
%            Maximize $I(\rho_i;\tau_j) + \mathcolorbox {green}{d_{\phi}(\tau_i,\tau_j)}$
%        \end{exampleblock}
%        \pause
%        \begin{alertblock}{Limit}
%            Might discard $d_{\phi}$
%        \end{alertblock}
%    \end{frame}
    \begin{frame}{Specialization ($\mathcal{L}_{D}$)}

        To encourage subtasks specialization:

        \begin{exampleblock}{Solution (dissimilarity and constrained program)}

            \begin{align*}
                & \text{Let}\ {D_\phi=(d_{\phi}(\tau_i,\tau_j))_{i,j}}\\
                &\mathcolorbox{orange}{\underset{{\theta_\rho, \xi, \phi}}{\text{minimize}}} \ \ \ \mathcolorbox {orange}{\|D_\phi^t\|_{0}} \\
                &\text{subject to} \ \ \ \mathcolorbox {yellow}{I(\rho^{t}_i; \tau^{t-1}_j|o^t_j)} + \mathcolorbox {orange}{d_\phi(\tau^{t-1}_i, \tau^{t-1}_j)} \mathcolorbox {yellow}{>U}, \forall i\ne j
            \end{align*}

%
        \end{exampleblock}

%        where $U$ controls the compactness of the embedding space.
        \pause
        \begin{block}{}
            Encourage \colorbox{yellow}{compactness} and \colorbox{orange}{diversity} ($U$ is the slider).
        \end{block}
        \pause
        \begin{alertblock}{Limit}
            Untractable
        \end{alertblock}
    \end{frame}

    \begin{frame}{Specialization ($\mathcal{L}_{D}$)}
        \begin{exampleblock}{Solution (\colorbox{cyan}{relaxation} and \colorbox{green}{variational posterior estimator})}
            \begin{align*}
                &\mathcal{L}_D(\theta_\rho, \phi, \xi) =
                \\ &\mathbb{E}_{(\bm{\tau}^{t-1}, \bm{o}^t)\sim\mathcal{D}, \bm{\rho}^t\sim p(\bm{\rho}^t | \bm{o}^t)}
                \\ &\big[\|D_\phi^t\|_{\mathcolorbox{cyan}{F}} - \sum_{i\ne j}\min\{\mathcolorbox{green}{q_\xi(\rho^{t}_i| \tau^{t-1}_j, o^t_j)} + d_\phi(\tau^{t-1}_i, \tau^{t-1}_j), U\}\big]
            \end{align*}
        \end{exampleblock}

    \end{frame}


    \begin{frame}{Including roles in the mixing architecture.}

        At each step:
        \begin{itemize}
            \pause\item Observation is encoded into role distribution parameters.
            \pause\item A role is sampled.
            \pause\item The role is decoded through an hypernet $\mathcolorbox{yellow}{g_{\theta_h}}$ then feed to the local utility network.
        \end{itemize}

    \end{frame}

%    \begin{exampleblock}{Solution}
%        Learn a dissimilarity model $d_{\phi}$ for trajectories:
%        \begin{itemize}
%            \item Maximise $I(\rho_i;\tau_j) + d_{\phi}(\tau_i,\tau_j)$
%        \end{itemize}
%    \end{exampleblock}

    \begin{frame}{Putting things togetheir}

        If $\theta=(\theta_\rho, \xi, \phi, \theta_h, \theta_m)$, $\lambda_I$ and $\lambda_D$ are scaling factors.

        \only<1>{
            \begin{block}{Final loss}
                $\mathcal{L}(\theta)= \mathcal{L}_{TD}(\theta)
                + \lambda_{I} \mathcal{L}_{I}(\theta_{\rho},\xi)
                + \lambda_{D}\mathcal{L}_{D}(\theta_{\rho},\xi,\phi)$\\
            \end{block}
            \begin{itemize}
                \item[] {}
            \end{itemize}
        }

        \only<2>{
            \begin{block}{Final loss}
                $\mathcal{L}(\theta)= \mathcolorbox {green}{\mathcal{L}_{TD}(\theta_m)}
                + \lambda_{I} \mathcal{L}_{I}(\theta_{\rho},\xi)
                + \lambda_{D}\mathcal{L}_{D}(\theta_{\rho},\xi,\phi)$
            \end{block}

            \begin{itemize}
                \item Temporal Difference loss \\$\mathcal{L}_{TD}(\theta_m)$ = $[r + \gamma \max_{\va'} Q_{tot}(s', \bm{a'}; \theta_m^-)$-$Q_{tot}$($s, \bm{a}; \theta_m$)$]^2$.
            \end{itemize}
        }
        \only<3>{
            \begin{block}{Final loss}
                $\mathcal{L}(\theta)= \mathcal{L}_{TD}(\theta_m)
                + \lambda_{I} \mathcolorbox {green}{\mathcal{L}_{I}(\theta_{\rho},\xi)}
                + \lambda_{D}\mathcal{L}_{D}(\theta_{\rho},\xi,\phi)$
            \end{block}

            \begin{itemize}
                \item Identifiability and versatility loss.
            \end{itemize}
        }
        \only<4>{
            \begin{block}{Final loss}
                $\mathcal{L}(\theta)= \mathcal{L}_{TD}(\theta_m)
                + \lambda_{I} \mathcal{L}_{I}(\theta_{\rho},\xi)
                + \lambda_{D}\mathcolorbox {green}{\mathcal{L}_{D}(\theta_{\rho},\xi,\phi)}$
            \end{block}

            \begin{itemize}
                \item Specialization loss.
            \end{itemize}
        }
    \end{frame}

    \begin{frame}{The framework}

        \begin{figure}
            \centering
            \includegraphics[width=1\linewidth]{img/framework.pdf}
            %\caption{Schematics of our approach. The role encoder generates a role embedding distribution, from which a role is sampled and serves as an input to the hyper-net. The hyper-net generates the parameters of the local utility network. Local utilities are fed into a mixing network to get an estimation of the global action value. During the interactions with other agents and the environment, individual trajectories are collected and fed into the trajectory encoder to get posterior estimations of the role distributions. The framework can be trained in an end-to-end fashion.}
            %\label{fig:framework}
        \end{figure}


    \end{frame}

    \begin{frame}{Some results}

        \begin{figure}
            \includegraphics[width=\linewidth]{img/fig-lc/learning_curve.pdf}
            %\caption{Comparison of our method against baseline algorithms. Results for more maps can be found in Appendix C.1.}\label{fig:performance-baselines}
        \end{figure}
        \pause
        \begin{figure}
            \includegraphics[width=\linewidth]{img/fig-lc/ablation.pdf}
            %\caption{Comparison of our method against ablations.}\label{fig:performance-ablations}
        \end{figure}


    \end{frame}

    \begin{frame}{Dynamic roles}
        \begin{figure}
            \centering
            \includegraphics[height=0.17\linewidth]{img/fig-dynamic_roles/10m_vs_11m-g1.pdf}\hfill
            \includegraphics[height=0.17\linewidth]{img/fig-dynamic_roles/10m_vs_11m-g2.pdf}\hfill
            \includegraphics[height=0.17\linewidth]{img/fig-dynamic_roles/10m_vs_11m-g3.pdf}\\
            \subfigure[$t$=$1$]{\includegraphics[width=0.32\linewidth]{img/fig-dynamic_roles/10m_vs_11m-r1-new.pdf}}\hfill
            \subfigure[$t$=$8$]{\includegraphics[width=0.32\linewidth]{img/fig-dynamic_roles/10m_vs_11m-r2-new.pdf}}\hfill
            \subfigure[$t$=$19$]{\includegraphics[width=0.32\linewidth]{img/fig-dynamic_roles/10m_vs_11m-r3-new.pdf}}
        \end{figure}
    \end{frame}

    \begin{frame}{Example of roles spaces}
        \begin{figure}
            \centering
            \includegraphics[width=0.32\linewidth]{img/fig-various_roles/various_roles-g1.pdf}\hfill
            \includegraphics[width=0.32\linewidth]{img/fig-various_roles/various_roles-g2.pdf}\hfill
            \includegraphics[width=0.32\linewidth]{img/fig-various_roles/various_roles-g3.pdf}
            \subfigure{\includegraphics[width=0.32\linewidth]{img/fig-various_roles/various_roles-r1.pdf}\label{fig:various_roles-6s4z_vs_10b30z}}\hfill
            \subfigure{\includegraphics[width=0.32\linewidth]{img/fig-various_roles/various_roles-r2.pdf}}\hfill
            \subfigure{\includegraphics[width=0.32\linewidth]{img/fig-various_roles/various_roles-r3.pdf}}
        \end{figure}


    \end{frame}

    \begin{frame}{Conclusion}

        \begin{itemize}
            \item Improving the mixing architectures on complex hierachical tasks,
            \item by using a stochastic roles embedding space.
            \item Learned trough 2 regularizations:
            \begin{itemize}
                \item Versatily+Identifiability sub-loss;
                \item Specialization sub-loss.
            \end{itemize}

        \end{itemize}


    \end{frame}


\end{document}

